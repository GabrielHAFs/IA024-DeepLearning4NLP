{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings\n",
        "\n",
        "Neste exercício iremos treinar uma rede neural similar a do Bengio 2003 para prever a próxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Linguagem\".\n",
        "\n",
        "Portanto, você deve implementar o modelo de linguagem inspirado no artigo do Bengio, para prever a próxima palavra usando rede com embeddings e duas camadas.\n",
        "Sugestão de alguns parâmetros:\n",
        "* context_size = 9\n",
        "* max_vocab_size = 3000\n",
        "* embedding_dim = 64\n",
        "* usar pontuação no vocabulário\n",
        "* descartar qualquer contexto ou target que não esteja no vocabulário\n",
        "* É esperado conseguir uma perplexidade da ordem de 50.\n",
        "* Procurem fazer asserts para garantir que partes do seu programa estão testadas\n",
        "\n",
        "Este enunciado não é fixo, podem mudar qualquer um dos parâmetros acima, mas procurem conseguir a perplexidade esperada ou menor.\n",
        "\n",
        "Gerem alguns frases usando um contexto inicial e depois deslocando o contexto e prevendo a próxima palavra gerando frases compridas para ver se está gerando texto plausível.\n",
        "\n",
        "Algumas dicas:\n",
        "- Inclua caracteres de pontuação (ex: `.` e `,`) no vocabulário.\n",
        "- Deixe tudo como caixa baixa (lower-case).\n",
        "- A escolha do tamanho do vocabulario é importante: ser for muito grande, fica difícil para o modelo aprender boas representações. Se for muito pequeno, o modelo apenas conseguirá gerar textos simples.\n",
        "- Remova qualquer exemplo de treino/validação/teste que tenha pelo menos um token desconhecido (ou na entrada ou na saída).\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso.\n",
        "\n",
        "Procure por `TODO` para entender onde você precisa inserir o seu código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "bb2a9a23-45e6-49d8-c4b1-10e499556dfd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Trecho inspirado no trabalho do aluno Elton Cardoso (https://github.com/EltonCN/IA024/blob/main/02-ModeloBengio/Aula_1_2_Exerc%C3%ADcio_Modelo_de_Linguagem_(Bengio_2003)_MLP_%2B_Embeddings.ipynb)\n",
        "\n",
        "if not os.path.isfile(\"67724.txt.utf-8\"):\n",
        "    !curl -LO https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "\n",
        "if not os.path.isfile(\"67725.txt.utf-8\"):\n",
        "    !curl -LO https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "e8bc2ffc-e8b0-4f96-be38-bf77f462ccef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "execution_count": 235,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text  = open(\"67724.txt.utf-8\",\"r\", encoding=\"utf8\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\", encoding=\"utf8\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "caf8cbc9-0526-447e-84e6-0eb71808ec13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample do texto após ser processado: \n",
            "\n",
            "---\n",
            "--para vós, não duvido; mas isto não é razão de que o seja para outros.\n",
            "---\n",
            "\n",
            "Tamanho da lista de parágrafos processados: 4892\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  # Aplicando lower case em todo o texto\n",
        "  text = text.lower()\n",
        "  # Removendo quebras de linha\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "  text = text.replace(\"\\n\\n\", \" \")\n",
        "  # Removendo caracteres especiais @, #, $, %, _, =, (, )\n",
        "  text = re.sub(r'[@#$%_=()]', '', text)\n",
        "  # Substituindo caracteres numéricos por [NUM]\n",
        "  text = re.sub(r'\\d+', '[num]', text)\n",
        "  # Removendo espaços múltiplos\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "cleaned_paragraphs = [clean_text(paragraph) for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "print(f\"\"\"Sample do texto após ser processado: \\n\n",
        "---\n",
        "{cleaned_paragraphs[150]}\n",
        "---\n",
        "\"\"\")\n",
        "\n",
        "print(f\"Tamanho da lista de parágrafos processados: {len(cleaned_paragraphs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "2d8f181f-38f3-4d3e-bea1-7e55cd39edc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12430"
            ]
          },
          "execution_count": 237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(r'\\w+|\\S', text.lower())) # Mantém pontuação\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiP7OCo9zJ_I",
        "outputId": "842f17fd-02c0-4507-fb13-8a5fe246491e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most frequent words: ['.', ',', '-', 'a', 'que', 'o', 'de', 'e', 'se', ';']\n",
            "Least frequent words: ['destemido', 'importar', 'mergulhou', 'áquelle', 'morta', 'reflexão', 'hei', 'lh', 'decisivo', 'sobe']\n",
            "3000\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 3000\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
        "print(\"Most frequent words:\", most_frequent_words[:10])\n",
        "print(\"Least frequent words:\", most_frequent_words[-10:])\n",
        "\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "b10b3d7a-ab34-4300-f12d-208326c71f4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3, 3, 19, 230, 2, 13, 0, 10, 40, 118, 13, 38, 278, 7, 5, 6, 400, 19, 203, 1]"
            ]
          },
          "execution_count": 239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(r'\\w+|\\S', sentence.lower())]\n",
        "\n",
        "encode_sentence(cleaned_paragraphs[150], vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE6Mt55m_z02",
        "outputId": "06f984e5-adcc-4213-8aef-fa0775715949"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3230.67s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torchtext in /home/gabss/.local/lib/python3.10/site-packages (0.17.1)\n",
            "Requirement already satisfied: torch==2.2.1 in /home/gabss/.local/lib/python3.10/site-packages (from torchtext) (2.2.1)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /home/gabss/.local/lib/python3.10/site-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: tqdm in /home/gabss/.local/lib/python3.10/site-packages (from torchtext) (4.65.0)\n",
            "Requirement already satisfied: numpy in /home/gabss/.local/lib/python3.10/site-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: requests in /home/gabss/.local/lib/python3.10/site-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (4.8.0)\n",
            "Requirement already satisfied: sympy in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (1.12)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: networkx in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (3.2.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (2.19.3)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: filelock in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (3.13.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.2.1->torchtext) (3.0.3)\n",
            "Requirement already satisfied: fsspec in /home/gabss/.local/lib/python3.10/site-packages (from torch==2.2.1->torchtext) (2024.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /home/gabss/.local/lib/python3.10/site-packages (from torchdata==0.7.1->torchtext) (2.2.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/gabss/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchtext) (12.4.99)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/gabss/.local/lib/python3.10/site-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gabss/.local/lib/python3.10/site-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabss/.local/lib/python3.10/site-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/gabss/.local/lib/python3.10/site-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3237.87s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /home/gabss/.local/lib/python3.10/site-packages (2.2.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: filelock in /home/gabss/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: fsspec in /home/gabss/.local/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: sympy in /home/gabss/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: networkx in /home/gabss/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/gabss/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/gabss/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/gabss/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3244.99s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scikit-learn in /home/gabss/.local/lib/python3.10/site-packages (1.4.1.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/gabss/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/gabss/.local/lib/python3.10/site-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3252.23s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in /home/gabss/.local/lib/python3.10/site-packages (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade torchtext\n",
        "%pip install --upgrade torch\n",
        "%pip install --upgrade scikit-learn\n",
        "%pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GuaraniDataset(Dataset):\n",
        "    def __init__(self, text, context_size, vocab):\n",
        "        self.text = text\n",
        "        self.context_size = context_size\n",
        "        self.vocab = vocab\n",
        "        self.input_target_pairs = self.prepare_dataset()\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        \"\"\"\n",
        "        Cria pares de entrada e alvo com base no tamanho da janela de contexto.\n",
        "        \"\"\"\n",
        "        input_target_pairs = []\n",
        "        for paragraph in self.text:\n",
        "            tokens = encode_sentence(paragraph, self.vocab)\n",
        "            if len(tokens) >= self.context_size:\n",
        "              for i in range(len(tokens) - self.context_size):\n",
        "                  context = tokens[i:i+self.context_size]\n",
        "                  target  = tokens[i+self.context_size]\n",
        "                  input_target_pairs.append((torch.tensor(context), torch.tensor(target)))\n",
        "        return input_target_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Retorna o tamanho do dataset.\n",
        "        \"\"\"\n",
        "        return len(self.input_target_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retorna um par de entrada e alvo no índice especificado, convertidos em tensores.\n",
        "        \"\"\"\n",
        "        return self.input_target_pairs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1aetOpmDeQd",
        "outputId": "cb617146-ab6f-4496-d01f-3f912b627f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---\n",
            "Conjunto Treinamento: 3913 [80%]\n",
            "---\n",
            "Conjunto Validação: 979 [20%]\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "context_size = 10\n",
        "# Dividindo os pares de entrada-alvo em conjuntos de treino e validação\n",
        "train_data, val_data = train_test_split(\n",
        "    cleaned_paragraphs,\n",
        "    test_size=0.2,\n",
        "    random_state=18)\n",
        "\n",
        "#Verificando o tamanho dos vetores\n",
        "print(f\"\"\"\n",
        "---\n",
        "Conjunto Treinamento: {len(train_data)} [{round((len(train_data) / len(cleaned_paragraphs)) * 100)}%]\n",
        "---\n",
        "Conjunto Validação: {len(val_data)} [{round((len(val_data) / len(cleaned_paragraphs)) * 100)}%]\n",
        "---\n",
        "\"\"\")\n",
        "\n",
        "train_data = GuaraniDataset(train_data, context_size, vocab)\n",
        "val_data = GuaraniDataset(val_data, context_size, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [],
      "source": [
        "batch_size   = 30\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
        "        \"\"\"\n",
        "        Inicializa o modelo de linguagem.\n",
        "        :param vocab_size: Tamanho do vocabulário.\n",
        "        :param embedding_dim: Dimensão dos embeddings de palavras.\n",
        "        :param context_size: Tamanho da janela de contexto (número de palavras de entrada).\n",
        "        :param hidden_dim: Dimensão da camada oculta.\n",
        "        \"\"\"\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.context_size  = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim    = hidden_dim\n",
        "        self.vocab_size    = vocab_size\n",
        "\n",
        "        self.embeddings = nn.Embedding((self.vocab_size + 1), self.embedding_dim)\n",
        "        self.linear1    = nn.Linear(self.context_size * self.embedding_dim, self.hidden_dim*3)\n",
        "        self.linear2    = nn.Linear(self.hidden_dim*3, self.hidden_dim*3)\n",
        "        self.linear3    = nn.Linear(self.hidden_dim*3, (self.vocab_size + 1))\n",
        "        self.relu       = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Passo forward do modelo.\n",
        "        \"\"\"\n",
        "        # Fase forward para a camada de word features\n",
        "        o = self.embeddings(inputs).view((-1, self.context_size * self.embedding_dim))\n",
        "        o = torch.flatten(o, start_dim=1)\n",
        "        o = self.linear1(o)\n",
        "        o = self.linear2(o)\n",
        "        o = self.relu(o)\n",
        "        return self.linear3(o)\n",
        "        \n",
        "    #def forward(self, inputs):\n",
        "    #    \"\"\"\n",
        "    #    Passo forward do modelo.\n",
        "    #    \"\"\"\n",
        "    #    # Fase forward para a camada de word features\n",
        "    #    o = self.embeddings(inputs)  # Concatena embeddings das palavras de contexto\n",
        "    #    o = torch.flatten(o, start_dim=1)\n",
        "    #    # Fase forward para a camada oculta\n",
        "    #    o = F.tanh(self.linear1(o))\n",
        "    #    # Fase forward para as unidades de saída\n",
        "    #    o = self.linear2(o)\n",
        "    #    log_probs = F.log_softmax(o, dim=1)\n",
        "    #    return log_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yjQ1KXOzJ_K",
        "outputId": "1cd7917d-6f55-4704-ff42-37d0c0d8e748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LanguageModel(\n",
            "  (embeddings): Embedding(3001, 64)\n",
            "  (linear1): Linear(in_features=640, out_features=1920, bias=True)\n",
            "  (linear2): Linear(in_features=1920, out_features=1920, bias=True)\n",
            "  (linear3): Linear(in_features=1920, out_features=3001, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 64                              # Dimensão do embedding -> Dimensão baseada no modelo de embeddings text-embedding-ada-002 da OpenAI\n",
        "hidden_dim    = embedding_dim * context_size    # Dimensão da camada oculta\n",
        "\n",
        "# Instância do modelo\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, hidden_dim)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmsD59TfzJ_K",
        "outputId": "7642f864-3d1d-4943-adf6-95d942be4a0c"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "output = model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um0lR4mNzJ_K",
        "outputId": "b027e729-50c8-46d1-8693-b83a904d65c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---\n",
            "tensor([2183,   63, 2350, 2495, 2731, 2186, 2447, 1198, 1374, 2560,  549, 2852,\n",
            "        1342,  377, 2596, 1729, 2269,  880, 2491, 1763,   77,    6, 1763, 1342,\n",
            "         727, 1042,   77,  693,  324, 2495])\n",
            "---\n",
            "tensor([ 301,    2,    6,    4,    4, 1762,    5, 2409,  953,   22,   46,  458,\n",
            "           2,    0,  841,  347,   80,   31,   23,   11,    0,  987,    6,   65,\n",
            "        1814, 2117,   12,    0,    1,    3])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"\n",
        "---\n",
        "{output.argmax(dim=1)}\n",
        "---\n",
        "{target}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Treinamento e Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "e8079404-20cb-4027-f000-b0c8bb292a56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (embeddings): Embedding(3001, 64)\n",
              "  (linear1): Linear(in_features=640, out_features=1920, bias=True)\n",
              "  (linear2): Linear(in_features=1920, out_features=1920, bias=True)\n",
              "  (linear3): Linear(in_features=1920, out_features=3001, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRwSPiwizJ_L",
        "outputId": "cf72ab2c-f2be-4d22-b699-e1a95c8edb85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10],         Loss: 3.5526,         Perplexity: 282.33,         Elapsed Time: 5.92 sec,         Validation Loss: 5.0972,         Validation Perplexity: 163.56\n",
            "Epoch [2/10],         Loss: 4.5566,         Perplexity: 167.53,         Elapsed Time: 5.95 sec,         Validation Loss: 4.9437,         Validation Perplexity: 140.28\n",
            "Epoch [3/10],         Loss: 3.7503,         Perplexity: 133.63,         Elapsed Time: 6.06 sec,         Validation Loss: 4.8328,         Validation Perplexity: 125.57\n",
            "Epoch [4/10],         Loss: 5.0449,         Perplexity: 110.19,         Elapsed Time: 6.16 sec,         Validation Loss: 4.7476,         Validation Perplexity: 115.31\n",
            "Epoch [5/10],         Loss: 3.7046,         Perplexity: 91.36,         Elapsed Time: 5.97 sec,         Validation Loss: 4.6759,         Validation Perplexity: 107.33\n",
            "Epoch [6/10],         Loss: 4.9820,         Perplexity: 75.80,         Elapsed Time: 5.95 sec,         Validation Loss: 4.6300,         Validation Perplexity: 102.51\n",
            "Epoch [7/10],         Loss: 4.4709,         Perplexity: 62.78,         Elapsed Time: 6.07 sec,         Validation Loss: 4.6187,         Validation Perplexity: 101.36\n",
            "Epoch [8/10],         Loss: 4.2151,         Perplexity: 51.46,         Elapsed Time: 5.91 sec,         Validation Loss: 4.5866,         Validation Perplexity: 98.16\n",
            "Epoch [9/10],         Loss: 3.5875,         Perplexity: 41.89,         Elapsed Time: 5.96 sec,         Validation Loss: 4.5991,         Validation Perplexity: 99.39\n",
            "Epoch [10/10],         Loss: 5.9222,         Perplexity: 33.68,         Elapsed Time: 5.88 sec,         Validation Loss: 4.5902,         Validation Perplexity: 98.51\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Definições iniciais\n",
        "epochs = 10\n",
        "lr = 0.01 # Defina uma taxa de aprendizado inicial\n",
        "criterion = nn.CrossEntropyLoss()  # Função de perda de entropia cruzada para classificação\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)  # Usando AdamW como otimizador\n",
        "\n",
        "# Loop de treinamento\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()  # Start time of the epoch\n",
        "    model.train()  # Coloca o modelo em modo de treinamento\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:  # Supondo que 'train_loader' é seu DataLoader de treino\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)  # Faz a predição\n",
        "        loss = criterion(outputs, labels)  # Calcula a perda\n",
        "\n",
        "        optimizer.zero_grad()  # Zera os gradientes acumulados\n",
        "        loss.backward()  # Propagação reversa\n",
        "        optimizer.step()  # Atualiza os pesos\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)  # Calcula a loss média por época\n",
        "    perplexity = np.exp(epoch_loss)  # Calcula a perplexidade para a época\n",
        "\n",
        "    end_time = time.time()  # End time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Duration of epoch\n",
        "\n",
        "    # Avaliação no conjunto de validação\n",
        "    model.eval()  # Coloca o modelo em modo de avaliação\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():  # Desativa o cálculo de gradientes\n",
        "        for inputs, labels in val_loader:  # Supondo que 'val_loader' é seu DataLoader de validação\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "    val_perplexity = np.exp(val_epoch_loss)  # Calcula a perplexidade para o conjunto de validação\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], \\\n",
        "        Loss: {loss.item():.4f}, \\\n",
        "        Perplexity: {perplexity:.2f}, \\\n",
        "        Elapsed Time: {epoch_duration:.2f} sec, \\\n",
        "        Validation Loss: {val_epoch_loss:.4f}, \\\n",
        "        Validation Perplexity: {val_perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [],
      "source": [
        "#text = \"\"\n",
        "#\n",
        "#def generate_text(model, vocab, text, max_length):\n",
        "#    \"\"\"TODO: implemente a função para gerar texto até atingir o max_length\"\"\"\n",
        "#\n",
        "#context = 5\n",
        "#max_length= 10\n",
        "#generate_text(text, max_length)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
