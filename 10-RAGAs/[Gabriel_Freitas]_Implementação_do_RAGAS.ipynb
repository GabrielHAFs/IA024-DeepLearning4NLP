{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5MoY3KltNFT"
      },
      "source": [
        "# Implementação do RAGAS\n",
        "\n",
        "- Implementar o RAGAS com o LLaMA-3 70B para avaliar a qualidade das 50 anotações do IIRC usadas no exercício passado.\n",
        "- O RAGAS considera context, question, answer, keys que estão disponíveis no conjunto de teste do IIRC.\n",
        "- Opcional:\n",
        "    - Avaliar as respostas do exercício da aula 9_10\n",
        "    - Usar multi agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMNjrcvOtJup"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet groq nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT9Mi_hT_GM3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import tqdm\n",
        "\n",
        "GROQ_API_KEY = \"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\"\n",
        "MODEL_NAME = \"llama3-70b-8192\"\n",
        "EMBEDDINGS_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "NUM_QUESTIONS = 50\n",
        "\n",
        "def groq_completion(client,\n",
        "                    messages,\n",
        "                    model=MODEL_NAME,\n",
        "                    max_tokens=1024):\n",
        "    return client.chat.completions.create(\n",
        "                model=model,\n",
        "                max_tokens=max_tokens,\n",
        "                messages=messages).choices[0].message.content\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "embeddings_model = SentenceTransformer(EMBEDDINGS_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f05mrXIcWWr"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8JyTWdOcWWr"
      },
      "source": [
        "### Loading and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t0ut9p4cWWr"
      },
      "outputs": [],
      "source": [
        "with open('test_questions.json', 'r') as file:\n",
        "\ttest_questions  = json.load(file)\n",
        "\n",
        "for i, item in enumerate(test_questions[:NUM_QUESTIONS]):\n",
        "    # Print the question\n",
        "    print(\"Question:\", item['question'])\n",
        "\n",
        "    if item['answer']['type'] == 'span':\n",
        "        print(\"Answer:\", item['answer']['answer_spans'][0]['text'])\n",
        "    elif item['answer']['type'] in ['value', 'binary']:\n",
        "        print(\"Answer:\", item['answer']['answer_value'])\n",
        "\n",
        "    context = '. '.join(context_item['text'] for context_item in item['context']) + '.'\n",
        "    print(f\"Context {i}: {context}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbgMbr7kAQJ0"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zIcvIpwFLQA"
      },
      "outputs": [],
      "source": [
        "# Implementação baseada na do aluno Otávio Cury Pontes\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "all_questions = []\n",
        "all_answers = []\n",
        "all_contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "    all_questions.append(question['question'])\n",
        "\n",
        "    if question['answer']['type'] == 'span':\n",
        "        all_answers.append(question['answer']['answer_spans'][0]['text'])\n",
        "    if question['answer']['type'] == 'value':\n",
        "        answer = question['answer']['answer_value'] + \" \" + question['answer']['answer_unit'] + '.'\n",
        "        all_answers.append(answer)\n",
        "    if question['answer']['type'] == 'binary':\n",
        "        answer = question['answer']['answer_value'] + '.'\n",
        "        all_answers.append(answer)\n",
        "\n",
        "    context = ''\n",
        "    for item in question['context']:\n",
        "        context += item['text'] + \" \"\n",
        "\n",
        "    all_contexts.append(context)\n",
        "\n",
        "max_sent_context = -1\n",
        "max_sent_answer = -1\n",
        "\n",
        "for i in range(50):\n",
        "    num_sentence_answer =  len(sent_tokenize(all_answers[i]))\n",
        "    if num_sentence_answer > max_sent_context:\n",
        "        max_sent_answer = num_sentence_answer\n",
        "\n",
        "    num_sentence_context = len(sent_tokenize(all_contexts[i]))\n",
        "    if num_sentence_context > max_sent_context:\n",
        "        max_sent_context = num_sentence_context\n",
        "\n",
        "print(f\"{len(all_questions)}, {len(all_answers)}, {len(all_contexts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY-dQ1TNcWWs"
      },
      "source": [
        "## RAGAS evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bKmpe92KjJJ"
      },
      "source": [
        "### Faithfulness\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEdBx5jEcN_c"
      },
      "outputs": [],
      "source": [
        "faithfulness_user_prompt = \"\"\"\n",
        "Context: {context}\n",
        "{statement}\n",
        "\"\"\"\n",
        "\n",
        "faithfulness_system_prompt = \"\"\"\n",
        "Consider the given context and following statements, then determine whether they are supported by the information present in the context.\n",
        "\n",
        "Provide a brief explanation for each statement before arriving at the verdict (Yes/No). The veredict should be a 'string' Yes or No\n",
        "\n",
        "Provide a final veredict for each statement in order at the end in the JSON format.\n",
        "Do not deviate from the specified format!\n",
        "\n",
        "[\n",
        "    {\n",
        "    \"statement\": ,\n",
        "    \"explanation\": ,\n",
        "    \"veredict\":\n",
        "    },\n",
        "    {\n",
        "    \"statement\": ,\n",
        "    \"explanation\": ,\n",
        "    \"veredict\":\n",
        "    },\n",
        "]\n",
        "\n",
        "The final answer should only be the required JSON and nothing else.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G-TXcCLcWWs",
        "outputId": "aa2a399f-c6b0-41aa-9101-813c029c460b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "    \"statement\": \"Zeus is known for being a sky and thunder god in Greek mythology.\",\n",
            "    \"explanation\": \"The context directly states that Zeus is known as the sky and thunder god, making this statement true.\",\n",
            "    \"veredict\": \"Yes\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "faithfulness_user_prompt = faithfulness_user_prompt.format(context = all_questions[0], statement = all_answers[0])\n",
        "\n",
        "FAITHFULNESS_MESSAGE=[\n",
        "    {\"role\": \"system\", \"content\": faithfulness_system_prompt},\n",
        "    {\"role\": \"user\", \"content\": faithfulness_user_prompt}\n",
        "]\n",
        "\n",
        "response = groq_completion(client,\n",
        "                    FAITHFULNESS_MESSAGE,\n",
        "                    model=MODEL_NAME,\n",
        "                    max_tokens=1024)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpRO8KNZcWWt"
      },
      "source": [
        "### Answer Relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqfLGLJWcWWt"
      },
      "outputs": [],
      "source": [
        "answer_relevance_system_prompt = \"\"\"\n",
        "Generate a question for the given answer.\n",
        "Provide your response in a JSON format with the following schema:\n",
        "\n",
        "{\n",
        "\"answer\": [answer],\n",
        "\"generated question\": [question]\n",
        "}\n",
        "\n",
        "The final answer should only be the required JSON and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "answer_relevance_user_prompt = \"\"\"\n",
        "Answer: {answer}\n",
        "generated question:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwGNYJ9QcWWt"
      },
      "outputs": [],
      "source": [
        "answer_relevance_user_prompt = answer_relevance_user_prompt.format(answer = all_answers[0])\n",
        "\n",
        "ANSWER_RELEVANCE_MESSAGE=[\n",
        "    {\"role\": \"system\", \"content\": answer_relevance_system_prompt},\n",
        "    {\"role\": \"user\", \"content\": answer_relevance_user_prompt}\n",
        "]\n",
        "\n",
        "response = groq_completion(client,\n",
        "                    ANSWER_RELEVANCE_MESSAGE,\n",
        "                    model=MODEL_NAME,\n",
        "                    max_tokens=1024)\n",
        "print(response)\n",
        "\n",
        "json_response = json.loads(response)\n",
        "\n",
        "generated_question = embeddings_model.encode(json_response['generated question'], convert_to_tensor=True)\n",
        "question = embeddings_model.encode(all_questions[0], convert_to_tensor=True)\n",
        "\n",
        "similarity = util.pytorch_cos_sim(generated_question, question)\n",
        "\n",
        "print(f\"\"\"\n",
        "Calculated similarity score: {similarity.item()}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QlqS_qncWWt",
        "outputId": "8e22c701-9175-49d9-b2bf-ea076e46c926"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:01<00:00,  2.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Answer Relevance score = 11.28%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "AR = 0.0\n",
        "N = len(all_questions)\n",
        "\n",
        "for i in tqdm(range(NUM_QUESTIONS)):\n",
        "    retry = 0\n",
        "\n",
        "    answer_relevance_user_prompt = answer_relevance_user_prompt.format(answer = all_answers[i])\n",
        "\n",
        "    ANSWER_RELEVANCE_MESSAGE=[\n",
        "        {\"role\": \"system\", \"content\": answer_relevance_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": answer_relevance_user_prompt}\n",
        "    ]\n",
        "\n",
        "    while retry < 10:\n",
        "        try:\n",
        "            response = json.loads(groq_completion(client,\n",
        "                    ANSWER_RELEVANCE_MESSAGE,\n",
        "                    model=MODEL_NAME,\n",
        "                    max_tokens=1024))\n",
        "            time.sleep(2)\n",
        "            generated_question = embeddings_model.encode(response['generated question'], convert_to_tensor=True)\n",
        "            question = embeddings_model.encode(all_questions[i], convert_to_tensor=True)\n",
        "\n",
        "            similarity = util.pytorch_cos_sim(generated_question, question)\n",
        "            AR += similarity.item()\n",
        "\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            retry += 1\n",
        "\n",
        "AR = (AR/N)*100\n",
        "\n",
        "print(f\"\\nAnswer Relevance score = {AR:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWXvSpj2cWWt"
      },
      "source": [
        "### Context Relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At9pNJHXcWWu"
      },
      "outputs": [],
      "source": [
        "context_relevance_system_prompt = \"\"\"\n",
        "Please extract relevant sentences from the provided context that can potentially help answer the following question.\n",
        "Provide your response in a valid JSON format with the following schema:\n",
        "\n",
        "{\n",
        "\"question\": [question],\n",
        "\"context_size\": [value]\n",
        "\"sentences\": [\n",
        "        sentence1,\n",
        "        sentence2,\n",
        "        ...\n",
        "    ]\n",
        "}\n",
        "\n",
        "The final answer should only be the required JSON and nothing else.\n",
        "\n",
        "Constraints:\n",
        "\n",
        "1. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return an empty JSON schema such as below:\n",
        "\n",
        "{\n",
        "\"question\": ,\n",
        "\"context_size\": ,\n",
        "\"sentences\":\n",
        "}\n",
        "\n",
        "2. While extracting DO NOT, IN ANY CIRCUMSTANCE, make any changes to sentences from given context.\n",
        "\n",
        "3. The \"context_size\" property should contain the number of sentences of the given context.\n",
        "\"\"\"\n",
        "\n",
        "context_relevance_user_prompt = \"\"\"\n",
        "Question: {question}\n",
        "Context:  {context}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T_wROMbcWWu"
      },
      "outputs": [],
      "source": [
        "context_relevance_user_prompt = context_relevance_user_prompt.format(question= all_questions[0] , context= all_contexts[0])\n",
        "\n",
        "CONTEXT_RELEVANCE_MESSAGE=[\n",
        "    {\"role\": \"system\", \"content\": context_relevance_system_prompt},\n",
        "    {\"role\": \"user\", \"content\": context_relevance_user_prompt}\n",
        "]\n",
        "\n",
        "response = groq_completion(client,\n",
        "                    CONTEXT_RELEVANCE_MESSAGE,\n",
        "                    model=MODEL_NAME,\n",
        "                    max_tokens=1024)\n",
        "print(response)\n",
        "\n",
        "response = json.loads(response)\n",
        "\n",
        "context_sentences = response['context_size']\n",
        "extracted_sentences = len(response['sentences'])\n",
        "\n",
        "Cr = extracted_sentences/context_sentences\n",
        "\n",
        "print(f\"\"\"\n",
        "Calculated relevance: {Cr}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jcUpkUacWWu",
        "outputId": "91357a03-45af-4267-a6b1-6bc9b478c738"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:08<00:00,  2.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Context Relevance score = 98.04%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "extracted_sentences = 0.0\n",
        "context_sentences = 0.0\n",
        "test = 0.0\n",
        "\n",
        "for i in tqdm(range(50)):\n",
        "    retry = 0\n",
        "\n",
        "    context_relevance_user_prompt = context_relevance_user_prompt.format(question= all_questions[i] , context= all_contexts[i])\n",
        "\n",
        "    CONTEXT_RELEVANCE_MESSAGE=[\n",
        "        {\"role\": \"system\", \"content\": context_relevance_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": context_relevance_user_prompt}\n",
        "    ]\n",
        "\n",
        "    while retry < 10:\n",
        "        try:\n",
        "            response = json.loads(groq_completion(client,\n",
        "                        CONTEXT_RELEVANCE_MESSAGE,\n",
        "                        model=MODEL_NAME,\n",
        "                        max_tokens=1024))\n",
        "            time.sleep(2)\n",
        "            context_sentences += response['context_size']\n",
        "            extracted_sentences += len(response['sentences'])\n",
        "            test += len(sent_tokenize(all_contexts[i]))\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            retry += 1\n",
        "\n",
        "CR = (extracted_sentences/context_sentences)*100\n",
        "\n",
        "print(f\"\\nContext Relevance score = {CR:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}